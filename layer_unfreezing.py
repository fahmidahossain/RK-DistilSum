# -*- coding: utf-8 -*-
"""layer_unfreezing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yG9AE_xwH-aeRm4XP04-CwRBmDc_ujXV
"""

from google.colab import drive
drive.mount('/content/drive')

pip install datasets evaluate transformers rouge-score nltk



!apt install git-lfs

import transformers
from transformers.utils import send_example_telemetry

print(transformers.__version__)
send_example_telemetry("summarization_notebook", framework="pytorch")

from evaluate import load
rouge_metric = load('rouge')
bleu_metric = load('bleu')

model = "t5-small"

from transformers import AutoTokenizer
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

model_path = '/content/drive/My Drive/Summerization/GSG_FINETUNE/model_epoch_18.0'
tokenizer = AutoTokenizer.from_pretrained(model_path)

model_T5 = AutoModelForSeq2SeqLM.from_pretrained(model_path).to('cuda')

class EWC:
    def __init__(self, model, fisher, params, device='cuda'):
        self.model = model
        self.device = device
        self.fisher = fisher  # Use the loaded Fisher matrix
        self.params = params  # The parameters saved from the earlier task

import torch
# Load the Fisher matrix
fisher_load_path = '/content/drive/My Drive/Summerization/EWC/fisher_matrix.pth'
fisher_matrix = torch.load(fisher_load_path)

# Load the model parameters
params_load_path = '/content/drive/My Drive/Summerization/EWC/params.pth'
model_params = torch.load(params_load_path)

# Use the loaded Fisher matrix and parameters
ewc = EWC(model_T5, fisher=fisher_matrix, params=model_params)

print("Fisher matrix and model parameters loaded successfully!")

pip install datasets

from datasets import load_dataset, DatasetDict

# Load the dataset from the CSV file
dataset = load_dataset('csv', data_files='/content/drive/My Drive/merged_clean_data.csv')['train']
#Slice the dataset to only include the first 2000 examples
#dataset = dataset.select(range(2000))

def add_prefix_to_findings(example):
    example['Findings'] = "summarize: " + example['Findings']
    return example

dataset = dataset.map(add_prefix_to_findings)
# Split the dataset into 80% train, 10% validation, and 10% test
split_dataset = dataset.train_test_split(test_size=0.2)
test_validation_split = split_dataset['test'].train_test_split(test_size=0.5)

# Create a DatasetDict
dataset_dict = DatasetDict({
    'train': split_dataset['train'],
    'validation': test_validation_split['train'],
    'test': test_validation_split['test']
})


# Display the DatasetDict structure
print(dataset_dict)

import numpy as np

def find_max_mean_percentile_length(column_name, percentile=90):
    lengths = [len(entry) for entry in dataset_dict['train'][column_name]]
    max_length = np.max(lengths)
    mean_length = np.mean(lengths)
    percentile_length = np.percentile(lengths, percentile)

    # Find the index where the length equals the maximum length
    max_length_index = np.where(lengths == max_length)

    print(f"Max length index in {column_name}: {max_length_index}")
    return max_length, mean_length, percentile_length

max_input_length, mean_input_length, percentile_input_length = find_max_mean_percentile_length('Findings')
max_target_length, mean_target_length, percentile_target_length = find_max_mean_percentile_length('Impression')

print('Max FINDINGS AND IMPRESSION LENGTH:', max_input_length, max_target_length)
print('Mean FINDINGS AND IMPRESSION LENGTH:', mean_input_length, mean_target_length)
print('90th Percentile FINDINGS AND IMPRESSION LENGTH:', percentile_input_length, percentile_target_length)

max_input_length = 512
max_target_length = 256  #covers 90 percent of the sample length reduces lost info

def preprocess_function(examples):
    inputs = [doc for doc in examples["Findings"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    labels = tokenizer(text_target=examples["Impression"], max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)

from transformers import Seq2SeqTrainer

class UnfreezeTrainer(Seq2SeqTrainer):
    def __init__(self, *args, total_epoch,initial_unfreeze, final_unfreeze, **kwargs):
        super().__init__(*args, **kwargs)
        self.total_epoch = total_epoch
        self.initial_unfreeze = initial_unfreeze
        self.final_unfreeze = final_unfreeze
        self.num_encoder_layers = self.get_encoder_layer_count()
        self.update_layers = False
        self.freeze_encoder_layers()

        # Unfreeze 10% of the encoder layers at the start of training
        self.unfreeze_encoder_layers(initial_unfreeze)  # Unfreeze 10% of layers
        print("10% of encoder layers unfrozen before training begins.")


    def freeze_encoder_layers(self):
        """Freezes all encoder layers of the model."""
        for name, param in self.model.named_parameters():
            if name.startswith("encoder"):  # Freeze only the encoder layers
                param.requires_grad = False
        print(f"All {self.num_encoder_layers} encoder layers are initially frozen.")


    def unfreeze_encoder_layers(self, percentage):
        """ Unfreezes the specified percentage of encoder layers.  """
        layers_to_unfreeze = int(self.num_encoder_layers * percentage)
        layers_unfrozen = 0

        for name, param in self.model.named_parameters():
            if name.startswith("encoder") and not param.requires_grad:
                param.requires_grad = True
                layers_unfrozen += 1
                print(f"Unfreezing encoder layer: {name}")
            if layers_unfrozen >= layers_to_unfreeze:
                break

    def get_encoder_layer_count(self):
        """
        Count how many encoder layers are in the model.
        """
        encoder_layer_count = 0
        for name, _ in self.model.named_parameters():
            if name.startswith("encoder"):
                encoder_layer_count += 1
        return encoder_layer_count


    def compute_loss(self, model, inputs, return_outputs=False):
        # Get the loss from the base class (check if it's a tuple or not)
        loss = super().compute_loss(model, inputs, return_outputs=True)

        # Check if the loss is a tuple and print debug information
        if isinstance(loss, tuple):
            loss, outputs = loss  # Unpack the loss and outputs




        current_unfreeze_layers = self.get_unfreeze_layers()




        if self.update_layers :
            self.update_layers = False
            self.unfreeze_encoder_layers(current_unfreeze_layers)
            print("Current current_unfreeze_layers:", current_unfreeze_layers)



        # Return both loss and outputs if needed, else only return loss
        if return_outputs:
            return loss, outputs
        else:
            return loss


    def get_unfreeze_layers(self):
        # Assuming the first callback is PrintMetricsCallback, get the current lambda_ewc
        for callback in self.callback_handler.callbacks:
            if isinstance(callback, PrintMetricsCallback):
                self.update_layers = callback.update_layers
                callback.update_layers = False
                return callback.current_unfreeze
        return 0.0  # Return 0.0 if PrintMetricsCallback is not found

import nltk
import numpy as np

nltk.download('punkt')

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Rouge expects a newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

    # decoded lebels
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

    # Compute ROUGE scores
    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)
    rouge_result = {key: value * 100 for key, value in rouge_result.items()}  # Convert to percentage

    # Compute BLEU score
    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])
    bleu_scores = {
        "bleu": bleu_result['bleu'] * 100,  # Cumulative BLEU score
        "bleu1": bleu_result['precisions'][0] * 100,  # BLEU-1 score
        "bleu2": bleu_result['precisions'][1] * 100,  # BLEU-2 score
        "bleu3": bleu_result['precisions'][2] * 100,  # BLEU-3 score
        "bleu4": bleu_result['precisions'][3] * 100,  # BLEU-4 score
    }
    # Add mean generated length
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result = {
        **rouge_result,
        **bleu_scores,
        "gen_len": np.mean(prediction_lens)
    }

    return {k: round(v, 4) for k, v in result.items()}

from transformers import TrainerCallback
import pandas as pd
import os



class PrintMetricsCallback(TrainerCallback):
    def __init__(self,saved_path,model_saved_path,batch_size,lr,initial_unfreeze,final_unfreeze,total_epoch, model, tokenizer):
        self.count=0
        self.each_epoch_log_dict={}
        self.saved_path=saved_path
        self.model_saved_path=model_saved_path
        self.model = model
        self.tokenizer = tokenizer
        self.batch_size=batch_size
        self.lr=lr
        self.initial_unfreeze=initial_unfreeze
        self.final_unfreeze=final_unfreeze
        self.unfreeze_range=final_unfreeze-initial_unfreeze
        self.total_epoch=total_epoch
        self.current_unfreeze=initial_unfreeze
        self.total_current_unfreeze=initial_unfreeze
        self.update_layers =False

    def on_log(self, args, state, control, logs=None, **kwargs):
        # Logs is a dictionary with metric names as keys
        if logs is not None:
            self.count +=1
            print(self.count)
            # Print each metric including loss
            print("Metrics at epoch end:")
            for key, value in logs.items():
                self.each_epoch_log_dict[key]=value

            if self.count%2==0:

                try:
                  os.makedirs(self.saved_path, exist_ok=True)
                except Exception as e:
                  print(f"Error creating directory {self.saved_path}: {e}")
                # Create the file path for the CSV
                csv_file = os.path.join(self.saved_path, f'epoch_log.csv')


                if os.path.exists(csv_file):
                    # Load the existing CSV file into a DataFrame
                    df = pd.read_csv(csv_file)
                    # Convert the dictionary to a DataFrame

                    new_df = pd.DataFrame([self.each_epoch_log_dict])
                    new_df.insert(0, 'Batch Size', self.batch_size)
                    new_df.insert(1, 'Learning Rate', self.lr)
                    new_df.insert(2, 'Unfreeze', self.total_current_unfreeze)
                    # Append the new DataFrame to the existing DataFrame
                    df = pd.concat([df,new_df], ignore_index=True)
                else:
                    # If the file does not exist, create a new DataFrame with the new data

                    df = pd.DataFrame([self.each_epoch_log_dict])
                    df.insert(0, 'Batch Size', self.batch_size)
                    df.insert(1, 'Learning Rate', self.lr)
                    df.insert(2, 'Unfreeze', self.total_current_unfreeze)

                new_unfreeze = self.gradually_increase_unfreeze(state.epoch, self.total_epoch)
                self.current_unfreeze = new_unfreeze
                self.update_layers = True
                self.total_current_unfreeze += new_unfreeze
                if self.total_current_unfreeze > 1:
                    self.total_current_unfreeze =1

                # Save the DataFrame to the CSV file

                df.to_csv(csv_file, index=False)
                print(f'New data has been added to {csv_file}.')

    def on_epoch_end(self, args, state, control, **kwargs):
            # Get the optimizer from kwargs if available
            optimizer = kwargs['optimizer']
            model_path = f"{self.model_saved_path}/epoch_{state.epoch}"


            try:
                os.makedirs(model_path, exist_ok=True)
            except Exception as e:
                print(f"Error creating directory {model_path}: {e}")
                return

            # Save model and tokenizer with learning rate in the filename
            try:
                self.model.save_pretrained(model_path)
                self.tokenizer.save_pretrained(model_path)
                print(f"Model and tokenizer saved to {model_path}")
            except Exception as e:
                print(f"Error saving model to {model_path}: {e}")


    def gradually_increase_unfreeze(self,current_epoch, total_epochs):

        start_unfreeze_percentage = self.initial_unfreeze
        current_unfreeze_percentage = (self.unfreeze_range * (1 / ((total_epochs - 1)- (0.2*total_epochs))))


        return current_unfreeze_percentage

batch_size = 16
learning_rate = 0.002
total_epoch = 20
initial_unfreeze = 0.1
final_unfreeze = 1



csv_saved_path= f'/content/drive/My Drive/Summerization/summerization_unfreeze_epoch/LOG'
model_saved_path= f'/content/drive/My Drive/Summerization/summerization_unfreeze_epoch/MODEL_EPOCH'

print_metrics_callback = PrintMetricsCallback(csv_saved_path,model_saved_path,batch_size,learning_rate,initial_unfreeze,final_unfreeze,total_epoch, model=model_T5, tokenizer=tokenizer)

args = Seq2SeqTrainingArguments(
        f"t5-small-finetuned-X-Ray_T5-lr-{learning_rate }",
        evaluation_strategy="epoch",
        logging_strategy="epoch",  # Log at specific intervals to capture training loss
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size ,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=total_epoch,
        predict_with_generate=True,
        fp16=True,
        push_to_hub=False,
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_T5)

trainer = UnfreezeTrainer(
    model=model_T5,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[ print_metrics_callback],
    total_epoch=total_epoch,
    initial_unfreeze=initial_unfreeze,
    final_unfreeze=final_unfreeze,

)

trainer.train()

initial_lambda = 0.5

# Manually loop through each epoch
for epoch in range(5):
    print(f"Starting epoch {epoch + 1}/{5}...")

    # Gradually decrease lambda_ewc
    lambda_ewc = gradually_decrease_lambda_ewc(initial_lambda, current_epoch=epoch, total_epochs=5)
    trainer.lambda_ewc = lambda_ewc
    print(f"Epoch {epoch + 1} lambda_ewc: {lambda_ewc}")

    # Train the model for one epoch
    trainer.train()

    # Evaluate the model after each epoch
    results = trainer.evaluate()
    print(f"Epoch {epoch + 1} results: {results}")

results = trainer.evaluate()
print(f"Epoch {epoch + 1} results: {results}")

model_T5.save_pretrained(model_saved_path)
tokenizer.save_pretrained(model_saved_path)

args = Seq2SeqTrainingArguments(
        f"t5-small-finetuned-X-Ray_T5-lr-{learning_rate }",
        evaluation_strategy="epoch",
        logging_strategy="epoch",  # Log at specific intervals to capture training loss
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size ,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=2,
        predict_with_generate=True,
        fp16=True,
        push_to_hub=False,
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_T5)

# Set up the callback
#drive_output_dir = '/content/drive/My Drive/Model'
#metrics_logger = MetricsLoggerCallback(output_dir=drive_output_dir, model=model_T5, tokenizer=tokenizer)

trainer = Seq2SeqTrainer(
        model=model_T5,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[ print_metrics_callback]
)

train_result = trainer.train()
eval_results = trainer.evaluate()

