# -*- coding: utf-8 -*-
"""decrease_lamda_ewc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FEwlFKG2ZeoTjcmTJfmzaCV4hn8F4MTu
"""

from google.colab import drive
drive.mount('/content/drive')

pip install datasets evaluate transformers rouge-score nltk



!apt install git-lfs

import transformers
from transformers.utils import send_example_telemetry

print(transformers.__version__)
send_example_telemetry("summarization_notebook", framework="pytorch")

from evaluate import load
rouge_metric = load('rouge')
bleu_metric = load('bleu')

model = "t5-small"

from transformers import AutoTokenizer
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

model_path = '/content/drive/My Drive/Summerization/GSG_FINETUNE/model_epoch_18.0'
tokenizer = AutoTokenizer.from_pretrained(model_path)

model_T5 = AutoModelForSeq2SeqLM.from_pretrained(model_path).to('cuda')

class EWC:
    def __init__(self, model, fisher, params, device='cuda'):
        self.model = model
        self.device = device
        self.fisher = fisher  # Use the loaded Fisher matrix
        self.params = params  # The parameters saved from the earlier task

import torch
# Load the Fisher matrix
fisher_load_path = '/content/drive/My Drive/Summerization/EWC/fisher_matrix.pth'
fisher_matrix = torch.load(fisher_load_path)

# Load the model parameters
params_load_path = '/content/drive/My Drive/Summerization/EWC/params.pth'
model_params = torch.load(params_load_path)

# Use the loaded Fisher matrix and parameters
ewc = EWC(model_T5, fisher=fisher_matrix, params=model_params)

print("Fisher matrix and model parameters loaded successfully!")

pip install datasets

from datasets import load_dataset, DatasetDict

# Load the dataset from the CSV file
dataset = load_dataset('csv', data_files='/content/drive/My Drive/merged_clean_data.csv')['train']
# Slice the dataset to only include the first 2000 examples
#dataset = dataset.select(range(2000))

def add_prefix_to_findings(example):
    example['Findings'] = "summarize: " + example['Findings']
    return example

dataset = dataset.map(add_prefix_to_findings)
# Split the dataset into 80% train, 10% validation, and 10% test
split_dataset = dataset.train_test_split(test_size=0.2)
test_validation_split = split_dataset['test'].train_test_split(test_size=0.5)

# Create a DatasetDict
dataset_dict = DatasetDict({
    'train': split_dataset['train'],
    'validation': test_validation_split['train'],
    'test': test_validation_split['test']
})


# Display the DatasetDict structure
print(dataset_dict)

import numpy as np

def find_max_mean_percentile_length(column_name, percentile=90):
    lengths = [len(entry) for entry in dataset_dict['train'][column_name]]
    max_length = np.max(lengths)
    mean_length = np.mean(lengths)
    percentile_length = np.percentile(lengths, percentile)

    # Find the index where the length equals the maximum length
    max_length_index = np.where(lengths == max_length)

    print(f"Max length index in {column_name}: {max_length_index}")
    return max_length, mean_length, percentile_length

max_input_length, mean_input_length, percentile_input_length = find_max_mean_percentile_length('Findings')
max_target_length, mean_target_length, percentile_target_length = find_max_mean_percentile_length('Impression')

print('Max FINDINGS AND IMPRESSION LENGTH:', max_input_length, max_target_length)
print('Mean FINDINGS AND IMPRESSION LENGTH:', mean_input_length, mean_target_length)
print('90th Percentile FINDINGS AND IMPRESSION LENGTH:', percentile_input_length, percentile_target_length)

max_input_length = 512
max_target_length = 256  #covers 90 percent of the sample length reduces lost info

def preprocess_function(examples):
    inputs = [doc for doc in examples["Findings"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    labels = tokenizer(text_target=examples["Impression"], max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)

from transformers import Seq2SeqTrainer

class EWCT5Trainer(Seq2SeqTrainer):
    def __init__(self, *args, ewc=None, lambda_ewc=0.4, **kwargs):
        super().__init__(*args, **kwargs)
        self.ewc = ewc
        self.lambda_ewc = lambda_ewc  # EWC penalty parameter
        self.print_lambda = 0

    def compute_loss(self, model, inputs, return_outputs=False):
        # Get the loss from the base class (check if it's a tuple or not)
        loss = super().compute_loss(model, inputs, return_outputs=True)

        # Check if the loss is a tuple and print debug information
        if isinstance(loss, tuple):
            loss, outputs = loss  # Unpack the loss and outputs



        # Initialize EWC loss to 0 for debugging purposes
        ewc_loss = 0

        # Add EWC regularization penalty if EWC is enabled
        if self.ewc is not None:
            # Dynamically get the current lambda_ewc from the callback
            current_lambda_ewc = self.get_lambda_ewc()

            # Compute EWC loss
            for name, param in model.named_parameters():

                fisher_term = self.ewc.fisher[name] * (param - self.ewc.params[name]) ** 2
                ewc_loss += fisher_term.sum()

            if self.print_lambda != current_lambda_ewc:
                self.print_lambda = current_lambda_ewc
                print("Current lambda_ewc:", current_lambda_ewc)


            # Add EWC loss to the main task loss
            loss = loss + self.lambda_ewc * ewc_loss


        # Return both loss and outputs if needed, else only return loss
        if return_outputs:
            return loss, outputs
        else:
            return loss


    def get_lambda_ewc(self):
        # Assuming the first callback is PrintMetricsCallback, get the current lambda_ewc
        for callback in self.callback_handler.callbacks:
            if isinstance(callback, PrintMetricsCallback):

                return callback.lambda_ewc
        return self.lambda_ewc

import nltk
import numpy as np

nltk.download('punkt')

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Rouge expects a newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

    # decoded lebels
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

    # Compute ROUGE scores
    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)
    rouge_result = {key: value * 100 for key, value in rouge_result.items()}  # Convert to percentage

    # Compute BLEU score
    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])
    bleu_scores = {
        "bleu": bleu_result['bleu'] * 100,  # Cumulative BLEU score
        "bleu1": bleu_result['precisions'][0] * 100,  # BLEU-1 score
        "bleu2": bleu_result['precisions'][1] * 100,  # BLEU-2 score
        "bleu3": bleu_result['precisions'][2] * 100,  # BLEU-3 score
        "bleu4": bleu_result['precisions'][3] * 100,  # BLEU-4 score
    }
    # Add mean generated length
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result = {
        **rouge_result,
        **bleu_scores,
        "gen_len": np.mean(prediction_lens)
    }

    return {k: round(v, 4) for k, v in result.items()}

from transformers import TrainerCallback
import pandas as pd
import os



class PrintMetricsCallback(TrainerCallback):
    def __init__(self,saved_path,model_saved_path,batch_size,lr,initial_lambda,total_epoch, model, tokenizer):
        self.count=0
        self.each_epoch_log_dict={}
        self.saved_path=saved_path
        self.model_saved_path=model_saved_path
        self.model = model
        self.tokenizer = tokenizer
        self.batch_size=batch_size
        self.lr=lr
        self.initial_lambda=initial_lambda
        self.total_epoch=total_epoch
        self.lambda_ewc=initial_lambda

    def on_log(self, args, state, control, logs=None, **kwargs):
        # Logs is a dictionary with metric names as keys
        if logs is not None:
            self.count +=1
            print(self.count)
            # Print each metric including loss
            print("Metrics at epoch end:")
            for key, value in logs.items():
                self.each_epoch_log_dict[key]=value

            if self.count%2==0:

                try:
                  os.makedirs(self.saved_path, exist_ok=True)
                except Exception as e:
                  print(f"Error creating directory {self.saved_path}: {e}")
                # Create the file path for the CSV
                csv_file = os.path.join(self.saved_path, f'epoch_log.csv')


                if os.path.exists(csv_file):
                    # Load the existing CSV file into a DataFrame
                    df = pd.read_csv(csv_file)
                    # Convert the dictionary to a DataFrame

                    new_df = pd.DataFrame([self.each_epoch_log_dict])
                    new_df.insert(0, 'Batch Size', self.batch_size)
                    new_df.insert(1, 'Learning Rate', self.lr)
                    new_df.insert(2, 'Lambda', self.lambda_ewc)
                    # Append the new DataFrame to the existing DataFrame
                    df = pd.concat([df,new_df], ignore_index=True)
                else:
                    # If the file does not exist, create a new DataFrame with the new data

                    df = pd.DataFrame([self.each_epoch_log_dict])
                    df.insert(0, 'Batch Size', self.batch_size)
                    df.insert(1, 'Learning Rate', self.lr)
                    df.insert(2, 'Lambda', self.lambda_ewc)

                new_lambda_ewc = self.gradually_decrease_lambda_ewc(self.initial_lambda, state.epoch, self.total_epoch)
                self.lambda_ewc = new_lambda_ewc

                df.to_csv(csv_file, index=False)
                print(f'New data has been added to {csv_file}.')

    def on_epoch_end(self, args, state, control, **kwargs):
            # Get the optimizer from kwargs if available
            optimizer = kwargs['optimizer']
            model_path = f"{self.model_saved_path}/epoch_{state.epoch}"


            try:
                os.makedirs(model_path, exist_ok=True)
            except Exception as e:
                print(f"Error creating directory {model_path}: {e}")
                return

            # Save model and tokenizer with learning rate in the filename
            try:
                self.model.save_pretrained(model_path)
                self.tokenizer.save_pretrained(model_path)
                print(f"Model and tokenizer saved to {model_path}")
            except Exception as e:
                print(f"Error saving model to {model_path}: {e}")


    def gradually_decrease_lambda_ewc(self,initial_lambda, current_epoch, total_epochs):

        lambda_ewc = initial_lambda * (1 - current_epoch / total_epochs)
        if lambda_ewc > 0.2:
            return lambda_ewc
        return 0.2

batch_size = 16
learning_rate = 0.002
total_epoch = 20
initial_lambda = .9



csv_saved_path= f'/content/drive/My Drive/Summerization/summerization_EWC_epoch/LOG'
model_saved_path= f'/content/drive/My Drive/Summerization/summerization_EWC_epoch/MODEL_EPOCH'

print_metrics_callback = PrintMetricsCallback(csv_saved_path,model_saved_path,batch_size,learning_rate,initial_lambda,total_epoch, model=model_T5, tokenizer=tokenizer)

args = Seq2SeqTrainingArguments(
        f"t5-small-finetuned-X-Ray_T5-lr-{learning_rate }",
        evaluation_strategy="epoch",
        logging_strategy="epoch",  # Log at specific intervals to capture training loss
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size ,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=total_epoch,
        predict_with_generate=True,
        fp16=True,
        push_to_hub=False,
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_T5)

trainer = EWCT5Trainer(
    model=model_T5,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[ print_metrics_callback],
    ewc=ewc,
    lambda_ewc=initial_lambda
)

trainer.train()

results = trainer.evaluate()
print(f"Epoch {epoch + 1} results: {results}")

model_T5.save_pretrained(model_saved_path)
tokenizer.save_pretrained(model_saved_path)

args = Seq2SeqTrainingArguments(
        f"t5-small-finetuned-X-Ray_T5-lr-{learning_rate }",
        evaluation_strategy="epoch",
        logging_strategy="epoch",  # Log at specific intervals to capture training loss
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size ,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=2,
        predict_with_generate=True,
        fp16=True,
        push_to_hub=False,
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_T5)

# Set up the callback
#drive_output_dir = '/content/drive/My Drive/Model'
#metrics_logger = MetricsLoggerCallback(output_dir=drive_output_dir, model=model_T5, tokenizer=tokenizer)

trainer = Seq2SeqTrainer(
        model=model_T5,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[ print_metrics_callback]
)

train_result = trainer.train()
eval_results = trainer.evaluate()

