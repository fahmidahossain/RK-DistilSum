# -*- coding: utf-8 -*-
"""Kth_Rouge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b6jk4S1Z53LI7gJxMdjikw8L-PWRWIqD
"""

from google.colab import drive
drive.mount('/content/drive')

pip install datasets evaluate transformers rouge-score nltk

!apt install git-lfs

import transformers
from transformers.utils import send_example_telemetry

print(transformers.__version__)
send_example_telemetry("summarization_notebook", framework="pytorch")
model = "t5-small"
from evaluate import load
rouge_metric = load('rouge')
bleu_metric = load('bleu')

"""

---




**Observing the length on the sentences in the findings and Impression**


---

"""

from datasets import load_dataset
dataset = load_dataset('csv', data_files='/content/drive/My Drive/merged_clean_data.csv')['train']


import nltk
import numpy as np
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

# Function to count sentences
def count_sentences(text):
    return len(sent_tokenize(text))


# Function to calculate min, max, and 80th percentile sentence counts
def calculate_sentence_stats(dataset):
    findings_sentence_counts = [count_sentences(findings) for findings in dataset['Findings']]
    impression_sentence_counts = [count_sentences(impression) for impression in dataset['Impression']]

    min_findings_sentences = min(findings_sentence_counts)
    max_findings_sentences = max(findings_sentence_counts)
    min_impression_sentences = min(impression_sentence_counts)
    max_impression_sentences = max(impression_sentence_counts)

    percentile_95_findings = np.percentile(findings_sentence_counts, 95)
    percentile_95_impression = np.percentile(impression_sentence_counts, 95)

    return {
        'min_findings_sentences': min_findings_sentences,
        'max_findings_sentences': max_findings_sentences,
        'min_impression_sentences': min_impression_sentences,
        'max_impression_sentences': max_impression_sentences,
        'percentile_95_findings': percentile_95_findings,
        'percentile_95_impression': percentile_95_impression
    }

# Calculate and print sentence stats for each split

stats = calculate_sentence_stats(dataset)
print(f"  Findings - Min sentences: {stats['min_findings_sentences']}, Max sentences: {stats['max_findings_sentences']}, 95th percentile: {stats['percentile_95_findings']}")
print(f"  Impression - Min sentences: {stats['min_impression_sentences']}, Max sentences: {stats['max_impression_sentences']}, 95th percentile: {stats['percentile_95_impression']}")
print()

"""**bold text**

---
masking 3 sentence out 9 . around 30percent is masked and 70 percent is left for context.


---


"""

from datasets import load_dataset, DatasetDict


# Load the dataset from the CSV file

def calculate_rouge_scores(findings, impression):
    sentences = findings.split('. ')
    F1_list = []

    for sentence in sentences:
        # Skip empty sentences
        if not sentence.strip():
            continue



        if not impression.strip():
            bleu_result = {'bleu': 0}
            result = {'rouge1': 0}
        else:
            # Calculate ROUGE scores
            result = rouge_metric.compute(predictions=[sentence], references=[impression])
            bleu_result = bleu_metric.compute(predictions=[sentence], references=[impression] )

        # Extract the ROUGE-1 F1 score
        rouge1 = result['rouge1']  # This should be a float
        bleu = bleu_result['bleu']
        F1 = ((rouge1*bleu)/(rouge1+bleu))*2 if (rouge1 + bleu) != 0 else 0

        F1_list.append((sentence, F1))

    # Sort sentences by ROUGE-1 F1 score in descending order
    return sorted(F1_list, key=lambda x: x[1], reverse=True)


# Function to preprocess data
def preprocess_data(example):
    findings = example['Findings']
    impression = example['Impression']

    # Calculate ROUGE scores for each sentence
    scored_sentences = calculate_rouge_scores(findings, impression)

    # Determine the number of sentences to mask
    num_sentences = len(findings.split('. '))
    num_to_mask = min(3, num_sentences-2)  # Mask up to 3 sentences
    if num_to_mask<=0 :
      num_to_mask=1

    # Select top 3 sentences
    top_sentences = [sentence for sentence, score in scored_sentences[:num_to_mask ]]
    top_sentences_str = '.'.join(top_sentences)

    # Mask top 3 sentences in the findings
    masked_findings = findings
    for sentence in top_sentences:
        masked_findings = masked_findings.replace(sentence, "[MASK]")

    return {
       'input': masked_findings.strip(),  # Remove any trailing spaces
       'output': top_sentences_str.strip()  # Remove any trailing spaces
    }

processed_dataset = dataset.map(preprocess_data)

new_dataset = dataset.from_dict({
    'Findings' : dataset['Findings'],
     'Impression' : dataset['Impression'],
    'input': processed_dataset['input'],
    'target': processed_dataset['output']
})

# Save the new dataset to a CSV file (optional)
new_dataset.to_csv('/content/drive/My Drive/masked_findings_and_top_sentences.csv')

